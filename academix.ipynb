{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd44177d-4e28-4bcc-9617-a1b9cb7f170d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f54a19-4f01-4be7-9345-36b8c25f5cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "pip_install(\n",
    "    \"-q\",\n",
    "    \"--extra-index-url\",\n",
    "    \"https://download.pytorch.org/whl/cpu\",\n",
    "    \"llama-index\",\n",
    "    \"faiss-cpu\",\n",
    "    \"pymupdf\",\n",
    "    \"langchain\",\n",
    "    \"llama-index-readers-file\",\n",
    "    \"llama-index-vector-stores-faiss\",\n",
    "    \"llama-index-llms-langchain\",\n",
    "    \"llama-index-llms-huggingface>=0.3.0,<0.3.4\",\n",
    "    \"llama-index-embeddings-huggingface>=0.3.0\",\n",
    ")\n",
    "pip_install(\"-q\", \"git+https://github.com/huggingface/optimum-intel.git\", \"git+https://github.com/openvinotoolkit/nncf.git\", \"datasets\", \"accelerate\", \"gradio\")\n",
    "pip_install(\"--pre\", \"-U\", \"openvino>=2024.2\", \"--extra-index-url\", \"https://storage.openvinotoolkit.org/simple/wheels/nightly\")\n",
    "pip_install(\"--pre\", \"-U\", \"openvino-tokenizers[transformers]>=2024.2\", \"--extra-index-url\", \"https://storage.openvinotoolkit.org/simple/wheels/nightly\")\n",
    "pip_install(\"-q\", \"--no-deps\", \"llama-index-llms-openvino>=0.3.1\", \"llama-index-embeddings-openvino>=0.2.1\", \"llama-index-postprocessor-openvino-rerank>=0.2.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a0d9665-bb1c-47a6-a607-b4cd2c4065eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM config will be updated\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "import io\n",
    "config_shared_path = Path(\"../../utils/llm_config.py\")\n",
    "config_dst_path = Path(\"llm_config.py\")\n",
    "\n",
    "if not config_dst_path.exists():\n",
    "    if config_shared_path.exists():\n",
    "        try:\n",
    "            os.symlink(config_shared_path, config_dst_path)\n",
    "        except Exception:\n",
    "            shutil.copy(config_shared_path, config_dst_path)\n",
    "    else:\n",
    "        r = requests.get(url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/utils/llm_config.py\")\n",
    "        with open(\"llm_config.py\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(r.text)\n",
    "elif not os.path.islink(config_dst_path):\n",
    "    print(\"LLM config will be updated\")\n",
    "    if config_shared_path.exists():\n",
    "        shutil.copy(config_shared_path, config_dst_path)\n",
    "    else:\n",
    "        r = requests.get(url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/utils/llm_config.py\")\n",
    "        with open(\"llm_config.py\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(r.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "872296fd-19df-4fe1-8632-f3d22595f1fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fea04a57848b4fdcaa974e0a930c5814",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=True, description='Prepare INT4 model')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f33de61bcfd24260aaff3376d2296507",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=False, description='Prepare INT8 model')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abcd2ed1d6a549a395a1c3bca836667b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=False, description='Prepare FP16 model')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import ipywidgets as widgets\n",
    "from notebook_utils import device_widget, optimize_bge_embedding\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "prepare_int4_model = widgets.Checkbox(\n",
    "    value=True,\n",
    "    description=\"Prepare INT4 model\",\n",
    "    disabled=False,\n",
    ")\n",
    "prepare_int8_model = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description=\"Prepare INT8 model\",\n",
    "    disabled=False,\n",
    ")\n",
    "prepare_fp16_model = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description=\"Prepare FP16 model\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "display(prepare_int4_model)\n",
    "display(prepare_int8_model)\n",
    "display(prepare_fp16_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a97dc443-3d03-4e3c-b4c0-2ddd3bcb0f56",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d55e7ed64d9343658f2a800d96ceb272",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=False, description='Enable AWQ')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "enable_awq = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description=\"Enable AWQ\",\n",
    "    disabled=not prepare_int4_model.value,\n",
    ")\n",
    "display(enable_awq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f20115-aa76-44fa-a1a0-c8ae64750526",
   "metadata": {},
   "source": [
    "## Download model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63f1ae06-e794-4999-b260-74c2d3dcb4fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "model_path = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "fp16_model_dir = Path(\"Llama-3.2-1B-Instruct\") / \"FP16\"\n",
    "int8_model_dir = Path(\"Llama-3.2-1B-Instruct\") / \"INT8_compressed_weights\"\n",
    "int4_model_dir = Path(\"Llama-3.2-1B-Instruct\") / \"INT4_compressed_weights\"\n",
    "\n",
    "\n",
    "def convert_to_fp16():\n",
    "    if (fp16_model_dir / \"openvino_model.xml\").exists():\n",
    "        return\n",
    "    export_command = \"optimum-cli export openvino --model {} --task text-generation-with-past --weight-format fp16 {}\".format(\n",
    "        model_path, str(fp16_model_dir)\n",
    "    )\n",
    "    display(Markdown(\"**Export command:**\"))\n",
    "    display(Markdown(f\"`{export_command}`\"))\n",
    "    ! $export_command\n",
    "\n",
    "\n",
    "def convert_to_int8():\n",
    "    if (int8_model_dir / \"openvino_model.xml\").exists():\n",
    "        return\n",
    "    int8_model_dir.mkdir(parents=True, exist_ok=True)\n",
    "    export_command = \"optimum-cli export openvino --model {} --task text-generation-with-past --weight-format int8 {}\".format(\n",
    "        model_path, str(int8_model_dir)\n",
    "    )\n",
    "    display(Markdown(\"**Export command:**\"))\n",
    "    display(Markdown(f\"`{export_command}`\"))\n",
    "    ! $export_command\n",
    "\n",
    "\n",
    "def convert_to_int4():\n",
    "    # Zephyr-specific compression configuration\n",
    "    compression_config = {\n",
    "        \"sym\": True,\n",
    "        \"group_size\": 64,\n",
    "        \"ratio\": 0.6\n",
    "    }\n",
    "    \n",
    "    if (int4_model_dir / \"openvino_model.xml\").exists():\n",
    "        return\n",
    "    \n",
    "    export_command = \"optimum-cli export openvino --model {} --task text-generation-with-past --weight-format int4\".format(model_path)\n",
    "    export_command += \" --group-size {} --ratio {} --sym\".format(\n",
    "        compression_config[\"group_size\"], \n",
    "        compression_config[\"ratio\"]\n",
    "    )\n",
    "    \n",
    "    if enable_awq.value:\n",
    "        export_command += \" --awq --dataset wikitext2 --num-samples 128\"\n",
    "    \n",
    "    export_command += \" \" + str(int4_model_dir)\n",
    "    display(Markdown(\"**Export command:**\"))\n",
    "    display(Markdown(f\"`{export_command}`\"))\n",
    "    ! $export_command\n",
    "\n",
    "\n",
    "if prepare_fp16_model.value:\n",
    "    convert_to_fp16()\n",
    "if prepare_int8_model.value:\n",
    "    convert_to_int8()\n",
    "if prepare_int4_model.value:\n",
    "    convert_to_int4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74f2a17b-9548-48b8-9f22-8ddaebb76b62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "fp16_model_dir = Path(\"Llama-3.2-1B-Instruct\") / \"FP16\"\n",
    "int8_model_dir = Path(\"Llama-3.2-1B-Instruct\") / \"INT8_compressed_weights\"\n",
    "int4_model_dir = Path(\"Llama-3.2-1B-Instruct\") / \"INT4_compressed_weights\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f23d55-85df-4910-98a7-0595898d286e",
   "metadata": {},
   "source": [
    "## Compress model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e71e565e-32c9-46cf-b273-02af208361a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of model with INT4 compressed weights is 920.42 MB\n"
     ]
    }
   ],
   "source": [
    "fp16_weights = fp16_model_dir / \"openvino_model.bin\"\n",
    "int8_weights = int8_model_dir / \"openvino_model.bin\"\n",
    "int4_weights = int4_model_dir / \"openvino_model.bin\"\n",
    "\n",
    "if fp16_weights.exists():\n",
    "    print(f\"Size of FP16 model is {fp16_weights.stat().st_size / 1024 / 1024:.2f} MB\")\n",
    "for precision, compressed_weights in zip([8, 4], [int8_weights, int4_weights]):\n",
    "    if compressed_weights.exists():\n",
    "        print(f\"Size of model with INT{precision} compressed weights is {compressed_weights.stat().st_size / 1024 / 1024:.2f} MB\")\n",
    "    if compressed_weights.exists() and fp16_weights.exists():\n",
    "        print(f\"Compression rate for INT{precision} model: {fp16_weights.stat().st_size / compressed_weights.stat().st_size:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b16d5c05-4c2a-4d90-b67e-283c7df1debb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11d037dc2ee048f0ac796291bb69cda3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Model Language:', options=('English', 'Chinese', 'Japanese'), value='English')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llm_config import (\n",
    "    SUPPORTED_EMBEDDING_MODELS,\n",
    "    SUPPORTED_RERANK_MODELS,\n",
    "    SUPPORTED_LLM_MODELS,\n",
    ")\n",
    "\n",
    "model_languages = list(SUPPORTED_LLM_MODELS)\n",
    "\n",
    "model_language = widgets.Dropdown(\n",
    "    options=model_languages,\n",
    "    value=model_languages[0],\n",
    "    description=\"Model Language:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "model_language"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f682e6-112b-4081-95ac-2c60c2f43d89",
   "metadata": {},
   "source": [
    "## Set embedding model configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "48c1a179-7f17-4b05-8f03-ba81bd2e8e9e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4f15a94e0af4a70ad95622c580f8726",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Embedding Model:', options=('bge-small-en-v1.5', 'bge-large-en-v1.5', 'bge-m3'), value='…"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_model_id = list(SUPPORTED_EMBEDDING_MODELS[model_language.value])\n",
    "\n",
    "embedding_model_id = widgets.Dropdown(\n",
    "    options=embedding_model_id,\n",
    "    value=embedding_model_id[0],\n",
    "    description=\"Embedding Model:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "embedding_model_id\n",
    "embedding_model_configuration = SUPPORTED_EMBEDDING_MODELS[model_language.value][embedding_model_id.value]\n",
    "print(f\"Selected {embedding_model_id.value} model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e7fe26aa-c5f4-4c0f-93ae-c736a83a1d29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "export_command_base = \"optimum-cli export openvino --model {} --task feature-extraction\".format(embedding_model_configuration[\"model_id\"])\n",
    "export_command = export_command_base + \" \" + str(embedding_model_id.value)\n",
    "\n",
    "if not Path(embedding_model_id.value).exists():\n",
    "    ! $export_command"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be63af22-dd83-42b0-9ee7-faed4ad0295b",
   "metadata": {},
   "source": [
    "## Set rerank model configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "60dc6c59-a2e6-43a4-989e-b459dce9b8ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected bge-reranker-v2-m3 model\n"
     ]
    }
   ],
   "source": [
    "rerank_model_id = list(SUPPORTED_RERANK_MODELS)\n",
    "\n",
    "rerank_model_id = widgets.Dropdown(\n",
    "    options=rerank_model_id,\n",
    "    value=rerank_model_id[0],\n",
    "    description=\"Rerank Model:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "rerank_model_id\n",
    "rerank_model_configuration = SUPPORTED_RERANK_MODELS[rerank_model_id.value]\n",
    "print(f\"Selected {rerank_model_id.value} model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1a97599d-5d11-40da-9b60-1a8085683f33",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model will be loaded to AUTO device for text embedding\n"
     ]
    }
   ],
   "source": [
    "export_command_base = \"optimum-cli export openvino --model {} --task text-classification\".format(rerank_model_configuration[\"model_id\"])\n",
    "export_command = export_command_base + \" \" + str(rerank_model_id.value)\n",
    "\n",
    "if not Path(rerank_model_id.value).exists():\n",
    "    ! $export_command\n",
    "embedding_device = device_widget()\n",
    "embedding_device\n",
    "print(f\"Embedding model will be loaded to {embedding_device.value} device for text embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5d572c81-423a-476d-a14d-594bee75579b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rerenk model will be loaded to AUTO device for text reranking\n",
      "LLM model will be loaded to CPU device for response generation\n"
     ]
    }
   ],
   "source": [
    "USING_NPU = embedding_device.value == \"NPU\"\n",
    "\n",
    "npu_embedding_dir = embedding_model_id.value + \"-npu\"\n",
    "npu_embedding_path = Path(npu_embedding_dir) / \"openvino_model.xml\"\n",
    "\n",
    "if USING_NPU and not Path(npu_embedding_dir).exists():\n",
    "    shutil.copytree(embedding_model_id.value, npu_embedding_dir)\n",
    "    optimize_bge_embedding(Path(embedding_model_id.value) / \"openvino_model.xml\", npu_embedding_path)\n",
    "rerank_device = device_widget()\n",
    "rerank_device\n",
    "print(f\"Rerenk model will be loaded to {rerank_device.value} device for text reranking\")\n",
    "llm_device = device_widget(\"CPU\", exclude=[\"NPU\"])\n",
    "llm_device\n",
    "print(f\"LLM model will be loaded to {llm_device.value} device for response generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "da70772f-4b1c-4cec-9b7a-984eecae63f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n",
      "[-0.003010430606082082, -0.011976574547588825, 0.04138164594769478, -0.03790159523487091, 0.0242850873619318]\n"
     ]
    }
   ],
   "source": [
    "from llama_index.embeddings.huggingface_openvino import OpenVINOEmbedding\n",
    "\n",
    "embedding_model_name = npu_embedding_dir if USING_NPU else embedding_model_id.value\n",
    "batch_size = 1 if USING_NPU else 4\n",
    "\n",
    "embedding = OpenVINOEmbedding(\n",
    "    model_id_or_path=embedding_model_name, embed_batch_size=batch_size, device=embedding_device.value, model_kwargs={\"compile\": False}\n",
    ")\n",
    "if USING_NPU:\n",
    "    embedding._model.reshape(1, 512)\n",
    "embedding._model.compile()\n",
    "\n",
    "embeddings = embedding.get_text_embedding(\"Hello World!\")\n",
    "print(len(embeddings))\n",
    "print(embeddings[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ffdfa0ed-704f-483f-995f-1e83b1693071",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b40bf520ee3b4a3e92eda5b681ee784d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Model to run:', options=('INT4',), value='INT4')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "available_models = []\n",
    "if int4_model_dir.exists():\n",
    "    available_models.append(\"INT4\")\n",
    "if int8_model_dir.exists():\n",
    "    available_models.append(\"INT8\")\n",
    "if fp16_model_dir.exists():\n",
    "    available_models.append(\"FP16\")\n",
    "\n",
    "model_to_run = widgets.Dropdown(\n",
    "    options=available_models,\n",
    "    value=available_models[0],\n",
    "    description=\"Model to run:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "model_to_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "672faffe-c935-4b6c-bcd9-8c318bdce9f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87997148238a4470b6c7fca9f49ea8bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Model:', index=19, options=('tiny-llama-1b-chat', 'llama-3.2-1b-instruct', 'llama-3.2-3b…"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_model_ids = [model_id for model_id, model_config in SUPPORTED_LLM_MODELS[model_language.value].items() if model_config.get(\"rag_prompt_template\")]\n",
    "\n",
    "llm_model_id = widgets.Dropdown(\n",
    "    options= llm_model_ids,\n",
    "    value=llm_model_ids[-1],\n",
    "    description=\"Model:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "llm_model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "28d454ca-0f9c-4d89-9cb7-7f2fa9bb0ca4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87997148238a4470b6c7fca9f49ea8bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Model:', index=1, options=('tiny-llama-1b-chat', 'llama-3.2-1b-instruct', 'llama-3.2-3b-…"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7861e543-2f73-410a-b00d-0037033eb196",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm_model_configuration = SUPPORTED_LLM_MODELS[model_language.value][llm_model_id.value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "688aad77-4cc4-4269-b7dc-9248969216ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from Llama-3.2-1B-Instruct/INT4_compressed_weights\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.openvino import OpenVINOLLM\n",
    "\n",
    "import openvino.properties as props\n",
    "import openvino.properties.hint as hints\n",
    "import openvino.properties.streams as streams\n",
    "\n",
    "\n",
    "if model_to_run.value == \"INT4\":\n",
    "    model_dir = int4_model_dir\n",
    "elif model_to_run.value == \"INT8\":\n",
    "    model_dir = int8_model_dir\n",
    "else:\n",
    "    model_dir = fp16_model_dir\n",
    "print(f\"Loading model from {model_dir}\")\n",
    "\n",
    "ov_config = {hints.performance_mode(): hints.PerformanceMode.LATENCY, streams.num(): \"1\", props.cache_dir(): \"\"}\n",
    "\n",
    "stop_tokens = llm_model_configuration.get(\"stop_tokens\")\n",
    "completion_to_prompt = llm_model_configuration.get(\"completion_to_prompt\")\n",
    "\n",
    "if \"GPU\" in llm_device.value and \"qwen2-7b-instruct\" in llm_model_id.value:\n",
    "    ov_config[\"GPU_ENABLE_SDPA_OPTIMIZATION\"] = \"NO\"\n",
    "\n",
    "if llm_model_id.value == \"red-pajama-3b-chat\" and \"GPU\" in core.available_devices and llm_device.value in [\"GPU\", \"AUTO\"]:\n",
    "    ov_config[\"INFERENCE_PRECISION_HINT\"] = \"f32\"\n",
    "\n",
    "llm = OpenVINOLLM(\n",
    "    model_id_or_path=str(model_dir),\n",
    "    context_window=3900,\n",
    "    max_new_tokens=2,\n",
    "    model_kwargs={\"ov_config\": ov_config, \"trust_remote_code\": True},\n",
    "    generate_kwargs={\"temperature\": 0.7, \"top_k\": 50, \"top_p\": 0.95},\n",
    "    completion_to_prompt=completion_to_prompt,\n",
    "    device_map=llm_device.value,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "eb989232-4673-4ee0-a356-ba6e13114cba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stop_tokens = llm_model_configuration.get(\"stop_tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7de87e89-015f-47a8-ae0a-1ca6488399e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.postprocessor.openvino_rerank import OpenVINORerank\n",
    "\n",
    "reranker = OpenVINORerank(model_id_or_path=rerank_model_id.value, device=rerank_device.value, top_n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2b2ae7b0-e4f6-4199-badb-24dbef76314d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.9/site-packages/gradio/utils.py:1002: UserWarning: Expected 3 arguments for function <function update_retriever at 0x7fea2fdb4550>, received 5.\n",
      "  warnings.warn(\n",
      "/opt/app-root/lib64/python3.9/site-packages/gradio/utils.py:1010: UserWarning: Expected maximum 3 arguments for function <function update_retriever at 0x7fea2fdb4550>, received 5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on public URL: https://dd68ddfbc7409155d4.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://dd68ddfbc7409155d4.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex, StorageContext\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core import Settings\n",
    "from llama_index.readers.file import PyMuPDFReader\n",
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from llama_index.core.node_parser import LangchainNodeParser\n",
    "import faiss\n",
    "import torch\n",
    "import gradio as gr\n",
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "# Initialize global variables\n",
    "query_engine = None\n",
    "index = None\n",
    "TEXT_SPLITERS = {\n",
    "    \"SentenceSplitter\": SentenceSplitter,\n",
    "    \"RecursiveCharacter\": RecursiveCharacterTextSplitter,\n",
    "}\n",
    "\n",
    "def default_partial_text_processor(partial_text: str, new_text: str):\n",
    "    \"\"\"\n",
    "    helper for updating partially generated answer, used by default\n",
    "\n",
    "    Params:\n",
    "      partial_text: text buffer for storing previosly generated text\n",
    "      new_text: text update for the current step\n",
    "    Returns:\n",
    "      updated text string\n",
    "\n",
    "    \"\"\"\n",
    "    partial_text += new_text\n",
    "    return partial_text\n",
    "\n",
    "\n",
    "text_processor = llm_model_configuration.get(\"partial_text_processor\", default_partial_text_processor)\n",
    "\n",
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __init__(self, token_ids):\n",
    "        self.token_ids = token_ids\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        for stop_id in self.token_ids:\n",
    "            if input_ids[0][-1] == stop_id:\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "if llm._tokenizer.pad_token is None:\n",
    "    llm._tokenizer.pad_token = llm._tokenizer.eos_token\n",
    "\n",
    "# Setup the models and settings\n",
    "if stop_tokens is not None:\n",
    "    if isinstance(stop_tokens[0], str):\n",
    "        stop_tokens = llm._tokenizer.convert_tokens_to_ids(stop_tokens)\n",
    "    stop_tokens = [StopOnTokens(stop_tokens)]\n",
    "\n",
    "# Configure embedding dimensions\n",
    "d = embedding._model.request.outputs[0].get_partial_shape()[2].get_length()\n",
    "Settings.embed_model = embedding\n",
    "\n",
    "# Configure LLM settings\n",
    "llm.max_new_tokens = 2048\n",
    "if stop_tokens is not None:\n",
    "    llm._stopping_criteria = StoppingCriteriaList(stop_tokens)\n",
    "Settings.llm = llm\n",
    "\n",
    "def create_vectordb(doc, spliter_name, chunk_size, chunk_overlap, vector_search_top_k, vector_rerank_top_n, run_rerank):\n",
    "    \"\"\"\n",
    "    Initialize a vector database from user uploaded document\n",
    "    \"\"\"\n",
    "    global query_engine\n",
    "    global index\n",
    "\n",
    "    if vector_rerank_top_n > vector_search_top_k:\n",
    "        gr.Warning(\"Search top k must >= Rerank top n\")\n",
    "\n",
    "    # Load and process the uploaded document\n",
    "    loader = PyMuPDFReader()\n",
    "    for document in doc:\n",
    "        documents = loader.load(file_path=document.name)\n",
    "    \n",
    "    # Configure text splitter\n",
    "    spliter = TEXT_SPLITERS[spliter_name](chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    if spliter_name == \"RecursiveCharacter\":\n",
    "        spliter = LangchainNodeParser(spliter)\n",
    "    \n",
    "    # Initialize FAISS vector store\n",
    "    faiss_index = faiss.IndexFlatL2(d)\n",
    "    vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "    # Create the index from documents\n",
    "    index = VectorStoreIndex.from_documents(\n",
    "        documents,\n",
    "        storage_context=storage_context,\n",
    "        transformations=[spliter],\n",
    "    )\n",
    "    \n",
    "    # Setup query engine\n",
    "    if run_rerank:\n",
    "        reranker.top_n = vector_rerank_top_n\n",
    "        query_engine = index.as_query_engine(streaming=True, similarity_top_k=vector_search_top_k, node_postprocessors=[reranker])\n",
    "    else:\n",
    "        query_engine = index.as_query_engine(streaming=True, similarity_top_k=vector_search_top_k)\n",
    "\n",
    "    return \"Vector database is Ready\"\n",
    "\n",
    "def update_retriever(vector_search_top_k, vector_rerank_top_n, run_rerank):\n",
    "    \"\"\"\n",
    "    Update retriever settings\n",
    "    \"\"\"\n",
    "    global query_engine\n",
    "    global index\n",
    "\n",
    "    if vector_rerank_top_n > vector_search_top_k:\n",
    "        gr.Warning(\"Search top k must >= Rerank top n\")\n",
    "\n",
    "    if run_rerank:\n",
    "        reranker.top_n = vector_rerank_top_n\n",
    "        query_engine = index.as_query_engine(streaming=True, similarity_top_k=vector_search_top_k, node_postprocessors=[reranker])\n",
    "    else:\n",
    "        query_engine = index.as_query_engine(streaming=True, similarity_top_k=vector_search_top_k)\n",
    "\n",
    "def bot(history, temperature, top_p, top_k, repetition_penalty, do_rag):\n",
    "    \"\"\"\n",
    "    Chatbot callback function\n",
    "    \"\"\"\n",
    "    llm.generate_kwargs = dict(\n",
    "        temperature=temperature,\n",
    "        do_sample=temperature > 0.0,\n",
    "        top_p=top_p,\n",
    "        top_k=top_k,\n",
    "        repetition_penalty=repetition_penalty,\n",
    "    )\n",
    "\n",
    "    partial_text = \"\"\n",
    "    if do_rag:\n",
    "        streaming_response = query_engine.query(history[-1][0])\n",
    "        for new_text in streaming_response.response_gen:\n",
    "            partial_text = text_processor(partial_text, new_text)\n",
    "            history[-1][1] = partial_text\n",
    "            yield history\n",
    "    else:\n",
    "        streaming_response = llm.stream_complete(history[-1][0])\n",
    "        for new_text in streaming_response:\n",
    "            partial_text = text_processor(partial_text, new_text.delta)\n",
    "            history[-1][1] = partial_text\n",
    "            yield history\n",
    "\n",
    "def request_cancel():\n",
    "    llm._model.request.cancel()\n",
    "\n",
    "if not Path(\"gradio_helper.py\").exists():\n",
    "    r = requests.get(url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/notebooks/llm-rag-langchain/gradio_helper.py\")\n",
    "    open(\"gradio_helper.py\", \"w\").write(r.text)\n",
    "\n",
    "\n",
    "from gradio_helper import make_demo\n",
    "\n",
    "# Create and launch the demo\n",
    "demo = make_demo(\n",
    "    load_doc_fn=create_vectordb,\n",
    "    run_fn=bot,\n",
    "    update_retriever_fn=update_retriever,\n",
    "    model_name=llm_model_id.value,\n",
    "    language=model_language.value,\n",
    ")\n",
    "\n",
    "try:\n",
    "    demo.queue().launch(share=True)\n",
    "except Exception:\n",
    "    demo.queue().launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829dae20-e496-4adb-8f63-b668c610c5f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
